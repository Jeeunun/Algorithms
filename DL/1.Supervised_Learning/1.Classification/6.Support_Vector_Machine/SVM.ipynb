{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SVM**\n",
    "\n",
    "## 개념\n",
    "SVM은 새로운 데이터가 입력되었을 때 기존 데이터를 활용해 `분류`하는 방법이다. \n",
    "패턴인식, 자료분석 등을 위한 지도학습 모델로 회귀와 분류 문제 해결에 사용되는 알고리즘이다. \n",
    "\n",
    "최대 마진 분류기(Maximal Margin Classifier) -> SVC -> SVM 로 확장\n",
    "\n",
    "### **최대 마진 분류기(MMC)**\n",
    "#### 초평면 (Hyperplane)?\n",
    "SVM은 훈련데이터를 비선형매핑(Mapping)을 통해 고차원으로 변환한다. 이 새로운 차원에서 초평면(Hyperplane)을 최적으로 분리하는 선형분리를 찾는다. 즉, 최적의 Decision Boundary(의사결정영역)을 찾는다. (=즉, 비선형관계를 고차원으로 변환시켜 선형관계로 만든 후, 의사결정영역을 찾는 것!!) <br>\n",
    "\n",
    "##### _왜 데이터를 고차원으로 보내는 것일까?_ <br>\n",
    "예시에서 A=[a,d] , B=[b,c] 벡터는 2차원에서 비선형관계(=non-linearly separable, 비선형분리)이나, 한 차원 높은 3차원으로 Mapping하게 되면 선형관계(=linear separable, 선형분리)가 된다. 따라서 큰 차원으로 적절하게 비선형 매핑을 이용하면, 두 개의 클래스를 가진 데이터는 초평면(hyperplane)에서 항상 분리 될 수 있다.\n",
    "\n",
    "SVM은 복잡한 비선형 의사결정 영역을 모형화 할 수 있어서 매우 정확하며, 다른 모델보다 Overfitting되는 경향이 적다. <br>\n",
    "\n",
    "![그림1](./svm1.png)\n",
    "\n",
    "##### _데이터가 선형으로 분리되는 경우(hard margin방법으로 설명)_ <br>\n",
    "데이터의 집합 D = (x_1, y_1),(x_2, y_2),,,,(x_|d|, y_|d|) 라고 할 때, x_i는 클래스 y_i에 대한 훈련용 튜플(tuple)이다. 각 y_i는 buys_computer=yes, buys_computer=no에 해당하는 +1,-1 둘 중 하나의 값을 갖는다.    \n",
    "\n",
    "- 1. _2차원 속 벡터 데이터들은 선형적으로 분리가 가능하다(linearly separable)._ <br>\n",
    "분리할 수 있는 직선은 무수히 많이 존재하지만, **최적의 직선(=분류오류를 최소화하는 직선)** 을 찾아야 한다. \n",
    "\n",
    "- 2. _그러면 3차원은?_ <br>\n",
    "3차원 이상에서는 직선이 아닌 최적의 평면(plane)을 찾아야 한다. 즉, **최적의 초평면(hyperplane)** 을 찾아야 한다! 이 초평면이 바로 Decision Boundary!!\n",
    "\n",
    "결국, **SVM은 MMH(Maximum Marginal Hyperplane,최대마진초평면)을 찾아 분리하는 방법이다**\n",
    "\n",
    "![그림2](./svm2.png)\n",
    "> 2차원에서는 데이러를 분류할 수 있는 `직선`이 많이 존재한다. \n",
    "\n",
    "![그림3](./svm3.png)\n",
    "> 하지만 데이터를 분류하는데 있어서 `직선보다 초평면`이 더 정확하게 분류할 것을 알 수 있다. \n",
    "\n",
    "- 3. _분리 초평면은 $W*X+b=0$으로 나타낸다_\n",
    "W = 가중치(W={w_1, w_2,,,,w_n}) (n은 속성수), b = 편향값(bias)\n",
    "\n",
    "위의 그림은 $A_1, A_2$에 대한 2차원공간. X =(x_1,x_2) 일때 x_1과 x_2는 각각 A_1,A_2의 속성값이다.<br>\n",
    "X=(x_1, x_2) 이고 x_1과 x_2는 속성 A_1, A_2 의 값이다. b는 추가적인 가중치 w_0 <br>\n",
    "즉, w_0+w_1*x_1+w_2*x_2 = 0\n",
    "\n",
    "분리평면(Decision Boundary)위에 있는 모든 값은 w_0+w_1*x_1+w_2*x_2 > 0 를 만족. <br>\n",
    "분리평면(Decision Boundary)아래에 있는 모든 값은 w_0+w_1*x_1+w_2*x_2 <> 0 를 만족. <br>\n",
    "\n",
    "가중치는 수정될 수 있고, 초평면은 다음과 같이 정의 <br>\n",
    "$H_1$ :  w_0+w_1*x_1+w_2*x_2 >= 1 for y_i=+1 <br>\n",
    "$H_2$ :  w_0+w_1*x_1+w_2*x_2 <= 1 for y_i=-1 <br>\n",
    "=> 결론 : \n",
    "![그림4](./svm4.png) <br>\n",
    "\n",
    "초평면 $H_1,H_2$ 위에 위치하는 모든 튜플은 결론의 식을 만족. 이를 바로 서포트 벡터(그림 속 빨간 테두리의 원들)라고 한다. 서포트벡터는 분류하기가 가장 어려운 튜플이며 분류에 대해 가장 많은 정보를 준다.\n",
    "\n",
    "![그림5](./svm5.png)\n",
    "\n",
    "- 4. _SVM은 어떻게 MMH와 서포트벡터를 찾는걸까?_\n",
    "    - KKT 조건과 라그랑지 방법을 이용하여 구할 수 있다. (?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 설명한 SVM방식은 하드마진(Hard Margin) 방법.<br>\n",
    "하드마진 방식은 매우 엄격하게 두 개의 클래스를 분리하는 분리초평면을 구하는 방법으로, 모든 입력 튜플은 이 초평면을 사이에 두고 무조건 한 클래스에 속해야 한다. \n",
    "\n",
    "문제 발생 : 몇 개의 노이즈로 인해 두 그룹을 구별하는 분리초평면을 잘 못구할 수 있고, 찾지 못하는 경우가 생긴다. 따라서 현실에서는 하드마진을 적용하기 힘들다.\n",
    "\n",
    "문제를 해결하기 위해 **소프트마진(Soft margin)** 이 등장\n",
    "\n",
    "#### 소프트마진이란?\n",
    "하드마진 기반이며 차이점은 서포트 벡터가 위치한 경계선에 약간의 여유변수(Slack variable)을 두는 것.\n",
    "\n",
    "![그림6](./svm6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _데이터가 선형으로 분리되지 않는 경우_ <br>\n",
    "![그림1](./svm1.png)\n",
    "비선형SVM은 선형SVM의 확장을 통해 만들 수 있다.\n",
    "\n",
    "비선형 매핑(Mapping)을 통해 고차원으로 변환하면 선형분리 초평면을 가지지만 MMH를 구하기 위한 계산 비용이 많이 든다. 이를 해결하기 위해 커널트릭(Kernel Trick)을 사용한다. \n",
    "\n",
    "- Kernel Trick\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "### SVC (Support Vector Classifier)\n",
    "모든 데이터가 초평면에 의해 두 영역으로 분류될 수 있는 것이 아니기 때문에, 최대 마진 분류기는 모든 데이터에 적용하기 어렵다는 단점이 있다. 또한 분리 초평면에 의해 데이터 클래스를 나누면 학습데이터를 완벽하게 분류하기 때문에 `과적합문제`와 `테스트데이터의 이상치민감문제`가 있다.\n",
    "\n",
    "따라서 이상치로부터 영향을 덜 받으면서 대부분의 학습데이터를 잘 분류할 수 있는 방식을 고안한 것이 바로 `SVC`. SVC는 최대 마진 분류기를 가지면서도 일부 관측지들이 마진이나 초평면의 반대쪽에 있는 것을 허용한다. **_즉, 매개변수 C의 크기를 조정하며 오차허용도를 조절_**\n",
    "\n",
    "만약 C > 0 이면 C이하의 관측치들이 초평면의 반대쪽에 존재하는 것을 허용한다. 그러므로 C가 증가하면 모델을 더 유연하게 만들어 마진 위반에 대한 허용도가 커진다. 반면 C값이 작아지면 마진 위반에 대한 허용도를 줄인다.\n",
    "\n",
    "************\n",
    "### SVM\n",
    "모든 데이터가 선형으로 영역이 나뉠 수 없다. \n",
    "\n",
    "따라서 클래스 경계가 비선형인 상황에서는 Kernel을 활용해 SVM을 사용하면 된다. <br>\n",
    "커널이란 두 관측치들의 유사성을 수량화하는 함수를 말한다. \n",
    "\n",
    "![그림7](./svm7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
